{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Friday Plan\n",
    "\n",
    "## To-do\n",
    "* Index match retailer_dict to merged_data\n",
    "\n",
    "## Requirements\n",
    "* Total web traffic per sector at daily granularity\n",
    "* YoY web traffic per sector at daily granularity\n",
    "* Top 20 most popular websites on a weekly granularity\n",
    "    * Include previous year position\n",
    "* November and December\n",
    "* Pull data every Weds covering the previous Sunday to Saturday\n",
    "\n",
    "## Dates\n",
    "\n",
    "| Reporting period | Deliver on |\n",
    "|:-----------:|:----------:|\n",
    "|24th October-30th October|3rd November|\n",
    "|31st October-6th November|10th November|\n",
    "|7th November-13th November|17th November|\n",
    "|14th November-20th November|24th November|\n",
    "|21st November-27th November|1st December|\n",
    "|28th November-4th December|8th December|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import JSON\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import xlsxwriter as xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading will cost 186 API hits\n"
     ]
    }
   ],
   "source": [
    "# OPEN API KEY\n",
    "with open('key', 'r') as key: \n",
    "    key = key.read()\n",
    "\n",
    "# OPEN URL LIST\n",
    "with open('urls', 'r') as urls:\n",
    "    urls = [x.strip() for x in urls.readlines()]\n",
    "\n",
    "# PRINT COST OF TOTAL DOWNLOAD\n",
    "print(f'Downloading will cost {len(urls)} API hits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST 28 DAYS TRAFFIC FOR DOMAIN IN 'URLS' (COST = 1 API HIT PER DOMAIN)\n",
    "for url in urls:\n",
    "     if os.path.exists(f'2021_json/{url}.json'):\n",
    "          pass\n",
    "     else:\n",
    "          with open(f\"2021_json/{url}.json\", \"w\") as outfile:\n",
    "               json.dump(requests.get(\n",
    "               f\"https://api.similarweb.com/v1/website/\"\n",
    "               f\"{url}/total-traffic-and-engagement/visits?api_key=\"\n",
    "               f\"{key}&country=gb&granularity=daily&\"\n",
    "               f\"main_domain_only=false&format=json\"   \n",
    "               ).json(), outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAILY TRAFFIC FOR DOMAIN IN 'URLS' (COST = 1 API HIT PER DOMAIN)\n",
    "for url in urls:\n",
    "     if os.path.exists(f'2020_json/{url}.json'):\n",
    "          pass\n",
    "     else:\n",
    "          with open(f\"2020_json/{url}.json\", \"w\") as outfile:\n",
    "               json.dump(requests.get(\n",
    "               f\"https://api.similarweb.com/v1/website/\"\n",
    "               f\"{url}/total-traffic-and-engagement/visits?api_key=\"\n",
    "               f\"{key}&start_date=2020-11&end_date=2020-12&\"\n",
    "               f\"country=gb&granularity=daily&\"\n",
    "               f\"main_domain_only=false&format=json\"   \n",
    "               ).json(), outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VARIABLES FOR CSV PROCESSING\n",
    "remove = ['country', 'granularity', 'mtd', 'main_domain_only',\n",
    "          'show_verified', 'state', 'page', 'format', \n",
    "          'start_date', 'end_date']\n",
    "merged_df = []\n",
    "\n",
    "# OPEN JSON AND APPEND TO DATAFRAME\n",
    "for url in urls:\n",
    "    with open(f\"2021_json/{url}.json\", \"r\") as infile:\n",
    "        df = json.load(infile)\n",
    "        df = pd.json_normalize(df, 'visits').assign(**df[\"meta\"][\"request\"])\n",
    "        df = df.drop(columns=remove)\n",
    "        merged_df.append(df)\n",
    "\n",
    "# FORMAT DATAFRAME\n",
    "merged_df = pd.concat(merged_df)\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "merged_df = merged_df.set_index('date')\n",
    "merged_df = merged_df.loc['2021-10-31':'2021-11-06'] # UPDATE WEEKLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE DATAFRAME FOR SECTOR INFO\n",
    "retailer_info = pd.read_csv('retailer_dict.csv')\n",
    "retailer_info = retailer_info.drop(columns='Retailer Name')\n",
    "retailer_info = retailer_info.rename(columns={\"Website\": \"domain\", \"Category\": \"sector\"})\n",
    "retailer_info = retailer_info.set_index('domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOIN DATAFRAMES\n",
    "merged_df = merged_df.reset_index(drop=False)\n",
    "merged_df = merged_df.set_index('domain')\n",
    "merged_df = merged_df.join(retailer_info)\n",
    "merged_df = merged_df.reset_index()\n",
    "merged_df = merged_df.set_index('date')\n",
    "merged_df.to_csv('2021_json/merged_data.csv', encoding='utf-8', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE DATAFRAMES TO CSV\n",
    "domain_df = merged_df.pivot(columns='domain',values=['visits'])\n",
    "domain_df = domain_df['visits']\n",
    "domain_df = domain_df.reset_index()\n",
    "domain_df = domain_df.set_index('date')\n",
    "domain_df.to_csv('2021_json/domain_data.csv',encoding='utf-8',index=True)\n",
    "sector_df = pd.pivot_table(merged_df,index='date',columns='sector',values='visits') \n",
    "sector_df.to_csv('2021_json/sector_data.csv',encoding='utf-8',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAILY TRAFFIC FOR 2020 TO CSV\n",
    "merged_df = []\n",
    "\n",
    "for url in urls:\n",
    "    with open(f\"2020_json/{url}.json\", \"r\") as infile:\n",
    "        df = json.load(infile)\n",
    "        df = pd.json_normalize(df, 'visits').assign(**df[\"meta\"][\"request\"])\n",
    "        df = df.drop(columns=remove)\n",
    "        merged_df.append(df)\n",
    "\n",
    "merged_df = pd.concat(merged_df)\n",
    "merged_df = merged_df.reset_index(drop=True)\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "merged_df = merged_df.set_index('date')\n",
    "merged_df = merged_df.loc['2020-11-01':'2020-12-31']\n",
    "merged_df = merged_df.reset_index(drop=False)\n",
    "merged_df = merged_df.set_index('domain')\n",
    "merged_df = merged_df.join(retailer_info)\n",
    "merged_df = merged_df.reset_index()\n",
    "merged_df = merged_df.set_index('date')\n",
    "merged_df.to_csv('2020_json/merged_2020.csv', encoding='utf-8', index=True)\n",
    "domain_df = merged_df.pivot(columns='domain',values=['visits'])\n",
    "domain_df = domain_df['visits']\n",
    "domain_df = domain_df.reset_index()\n",
    "domain_df = domain_df.set_index('date')\n",
    "domain_df.to_csv('2020_json/domain_2020.csv',encoding='utf-8',index=True)\n",
    "sector_df = pd.pivot_table(merged_df,index='date',columns='sector',values='visits') \n",
    "sector_df.to_csv('2020_json/sector_2020.csv',encoding='utf-8',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST LAST 28 DAYS TRAFFIC TO JSON (COST = 1 API HIT)\n",
    "url = (f\"https://api.similarweb.com/v1/website/bbc.com/\"\n",
    "       f\"total-traffic-and-engagement/visits?api_key=\"\n",
    "       f\"{key}&country=world&granularity=daily&\"\n",
    "       f\"main_domain_only=false&format=json\")\n",
    "\n",
    "response = requests.request(\"GET\", url, headers={}, data={})\n",
    "\n",
    "with open(f\"{datetime.utcnow()}.json\", \"w\") as outfile:\n",
    "    json.dump(response.json(), outfile, indent=4)\n",
    "\n",
    "JSON(response.json())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d4e648b731ea9de700575cdc173c756b015a6aa3baf983218901678fd69ca6d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
